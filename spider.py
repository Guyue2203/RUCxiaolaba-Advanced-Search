#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import csv
import time
import os
import json
import pandas as pd
from datetime import datetime, timedelta
import re

# è®¾ç½®å·¥ä½œç›®å½•
os.chdir(os.path.dirname(os.path.abspath(__file__)))

print("ğŸ•·ï¸ å¯åŠ¨å°å–‡å­æ•°æ®çˆ¬è™«...")

def get_config_file():
    """è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    return './data/crawl_config.json'

def load_crawl_config():
    """åŠ è½½çˆ¬å–é…ç½®"""
    config_file = get_config_file()
    default_config = {
        "last_crawl_id": 0,
        "start_date": "2024-09-01",  # ä»ä¹æœˆä»½å¼€å§‹
        "last_crawl_time": None,
        "total_crawled": 0
    }
    
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                print(f"ğŸ“‹ åŠ è½½çˆ¬å–é…ç½®: ä¸Šæ¬¡çˆ¬å–ID={config.get('last_crawl_id', 0)}")
                return config
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    
    return default_config

def save_crawl_config(config):
    """ä¿å­˜çˆ¬å–é…ç½®"""
    config_file = get_config_file()
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜çˆ¬å–é…ç½®: å½“å‰ID={config['last_crawl_id']}")
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")

def get_start_id():
    """è·å–å¼€å§‹çˆ¬å–çš„ID"""
    config = load_crawl_config()
    return config['last_crawl_id']

def get_newest_id():
    """è·å–æœ€æ–°å¸–å­ID"""
    try:
        # ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹
        url = "https://ruc.yunshangxiaoyuan.cn/xiaoyuanapi/treeholeuser/getPostList"
        
        headers = {
            "Accept": "application/json, text/plain, */*",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090c11) XWEB/11275 Flue",
            "Content-Type": "application/json;charset=UTF-8",
            "Origin": "https://ruc.yunshangxiaoyuan.cn",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Dest": "empty",
            "Referer": "https://ruc.yunshangxiaoyuan.cn/treehole/20220429RUC",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Connection": "close",
        }
        
        payload = {
            "directCode": "20220429RUC",
            "classCode": "0",
            "typeId": 4,
            "lastId": 0,  # ä»æœ€æ–°çš„å¼€å§‹
            "openid": "ogEqTwqw93-HBCQ4dhnNKoluQSuc",
            "keywords": "",
            "sessionId": "",
            "direct": "20220429RUC",
        }
        
        print(f"ğŸ” å°è¯•è¿æ¥: {url}")
        response = requests.post(url, headers=headers, json=payload, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            post_list = data.get("postList", [])
            if post_list:
                newest_id = post_list[0].get("id", 0)
                print(f"âœ… è·å–åˆ°æœ€æ–°å¸–å­ID: {newest_id}")
                return newest_id
            else:
                print("âš ï¸ APIè¿”å›ç©ºæ•°æ®")
        else:
            print(f"âš ï¸ HTTPçŠ¶æ€ç : {response.status_code}")
            
    except requests.exceptions.ConnectionError as e:
        print(f"âŒ è¿æ¥é”™è¯¯: {e}")
    except requests.exceptions.Timeout as e:
        print(f"âŒ è¯·æ±‚è¶…æ—¶: {e}")
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
    
    # å¦‚æœAPIå¤±è´¥ï¼Œè¿”å›æ¨¡æ‹ŸIDç”¨äºæµ‹è¯•
    print("âš ï¸ APIè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
    return 100000  # è¿”å›ä¸€ä¸ªè¾ƒå¤§çš„æ¨¡æ‹ŸID

def generate_mock_data(start_id, end_id):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    print(f"ğŸ­ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® ID {start_id} åˆ° {end_id}")
    
    mock_posts = []
    categories = ['å­¦ä¹ ', 'ç”Ÿæ´»', 'ç¤¾å›¢', 'æ±‚èŒ', 'äºŒæ‰‹', 'å…¶ä»–']
    
    for i in range(start_id, min(start_id + 50, end_id)):  # é™åˆ¶ç”Ÿæˆæ•°é‡
        # ç”Ÿæˆéšæœºå¸–å­
        post = {
            'id': i,
            'content': f'è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•å¸–å­ï¼Œå†…å®¹åŒ…å«ä¸€äº›å…³é”®è¯å¦‚"å¤§å®¶"ã€"å­¦ä¹ "ã€"ç”Ÿæ´»"ç­‰ã€‚',
            'post_code': f'202409{i:05d}',
            'class_code': f'{i % 6 + 1:03d}',
            'class_name': categories[i % len(categories)],
            'time': f'2024-09-{(i % 30) + 1:02d} {i % 24:02d}:{i % 60:02d}:00',
            'good_count': i % 20,
            'comment_count': i % 10,
            'root_code': None if i % 3 == 0 else i - (i % 3)  # æ¨¡æ‹Ÿè¯„è®ºå…³ç³»
        }
        mock_posts.append(post)
    
    print(f"âœ… ç”Ÿæˆäº† {len(mock_posts)} æ¡æ¨¡æ‹Ÿæ•°æ®")
    return mock_posts

def crawl_posts(start_id, end_id):
    """çˆ¬å–æŒ‡å®šèŒƒå›´çš„å¸–å­ï¼ˆä»é«˜IDå‘ä½IDçˆ¬å–ï¼Œå› ä¸ºAPIçš„lastIdå‚æ•°å·¥ä½œåŸç†ï¼‰"""
    all_posts = []
    current_id = end_id  # ä»æœ€é«˜IDå¼€å§‹
    
    print(f"ğŸ“¥ å¼€å§‹çˆ¬å–å¸–å­ ID {start_id} åˆ° {end_id}")
    
    # æ­£å¼çˆ¬å–æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰æ•°æ®
    print(f"ğŸ“Š éœ€è¦çˆ¬å– {end_id - start_id} ä¸ªå¸–å­ID")
    
    # ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹å’Œè¯·æ±‚å¤´
    headers = {
        "Accept": "application/json, text/plain, */*",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090c11) XWEB/11275 Flue",
        "Content-Type": "application/json;charset=UTF-8",
        "Origin": "https://ruc.yunshangxiaoyuan.cn",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Dest": "empty",
        "Referer": "https://ruc.yunshangxiaoyuan.cn/treehole/20220429RUC",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Connection": "close",
    }

    root_url = "https://ruc.yunshangxiaoyuan.cn/xiaoyuanapi/treeholeuser/getPostList"
    comment_url = "https://ruc.yunshangxiaoyuan.cn/xiaoyuanapi/treeholeuser/getPostCommentList"

    crawl_count = 0
    max_attempts = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    
    while current_id > start_id:
        print(f"ğŸ”„ çˆ¬å–å¸–å­ID: {current_id} (å·²çˆ¬å–: {crawl_count} æ¡)")
        
        attempt = 0
        success = False
        
        while attempt < max_attempts and not success:
            try:
                # çˆ¬å–æ ¹å¸–å­
                payload = {
                    "directCode": "20220429RUC",
                    "classCode": "0",
                    "typeId": 4,
                    "lastId": current_id,
                    "openid": "ogEqTwqw93-HBCQ4dhnNKoluQSuc",
                    "keywords": "",
                    "sessionId": "",
                    "direct": "20220429RUC",
                }
                
                response = requests.post(root_url, headers=headers, json=payload, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    post_list = data.get("postList", [])
                    
                    if not post_list:
                        print(f"âš ï¸ è·³è¿‡IDä¸º{current_id}çš„å¸–å­ï¼ˆæ— æ•°æ®ï¼‰")
                        current_id -= 1
                        success = True  # æ ‡è®°ä¸ºæˆåŠŸï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                        continue
                    
                    # å¤„ç†æ¯ä¸ªæ ¹å¸–å­
                    for post in post_list:
                        # æ·»åŠ æ ¹å¸–å­
                        all_posts.append({
                            'id': post.get("id"),
                            'content': post.get("content"),
                            'post_code': post.get("post_code"),
                            'class_code': post.get("class_code"),
                            'class_name': post.get("class_name"),
                            'time': post.get("time"),
                            'good_count': post.get("good_count"),
                            'comment_count': post.get("comment_count"),
                            'root_code': None
                        })
                        
                        print(f"âœ… è·å–æ ¹å¸–å­ {post.get('post_code')}")
                        
                        # çˆ¬å–è¯¥å¸–å­çš„è¯„è®º
                        root_code = post.get("post_code")
                        comment_newest_id = 0
                        
                        while True:
                            comment_payload = {
                                "rootCode": root_code,
                                "lastId": comment_newest_id,
                                "openid": "ogEqTwqw93-HBCQ4dhnNKoluQSuc",
                                "sessionId": "",
                                "directCode": "20220429RUC",
                                "direct": "20220429RUC",
                            }
                            
                            try:
                                comment_response = requests.post(comment_url, headers=headers, json=comment_payload, timeout=30)
                                
                                if comment_response.status_code == 200:
                                    comment_data = comment_response.json()
                                    comment_list = comment_data.get("commentList", [])
                                    
                                    if not comment_list:
                                        break  # æ²¡æœ‰æ›´å¤šè¯„è®º
                                    
                                    # æ·»åŠ è¯„è®º
                                    for comment in comment_list:
                                        all_posts.append({
                                            'id': comment.get("id"),
                                            'content': comment.get("content"),
                                            'post_code': comment.get("post_code"),
                                            'class_code': comment.get("class_code"),
                                            'class_name': comment.get("class_name"),
                                            'time': comment.get("time"),
                                            'good_count': comment.get("good_count"),
                                            'comment_count': comment.get("comment_count"),
                                            'root_code': root_code
                                        })
                                    
                                    print(f"âœ… è·å– {len(comment_list)} æ¡è¯„è®º")
                                    
                                    # æ›´æ–°è¯„è®ºåˆ†é¡µID
                                    comment_newest_id = comment_list[-1].get("id")
                                    
                                else:
                                    print(f"âš ï¸ è¯„è®ºè¯·æ±‚å¤±è´¥: {comment_response.status_code}")
                                    break
                                    
                            except Exception as e:
                                print(f"âŒ è¯„è®ºè¯·æ±‚å¼‚å¸¸: {e}")
                                break
                            
                            time.sleep(0.08)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                    
                    # æ›´æ–°æ ¹å¸–å­åˆ†é¡µIDï¼ˆå‘ä½IDæ–¹å‘ç§»åŠ¨ï¼‰
                    current_id = post_list[-1].get("id") - 1
                    print(f"ğŸ“„ æ›´æ–°åˆ†é¡µIDè‡³: {current_id}")
                    success = True
                    crawl_count += 1
                    
                else:
                    print(f"âš ï¸ æ ¹å¸–å­è¯·æ±‚å¤±è´¥: {response.status_code}")
                    attempt += 1
                    if attempt < max_attempts:
                        print(f"ğŸ”„ é‡è¯•ç¬¬ {attempt} æ¬¡...")
                        time.sleep(0.4)
                    else:
                        current_id -= 1
                        success = True  # è·³è¿‡è¿™ä¸ªID
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                attempt += 1
                if attempt < max_attempts:
                    print(f"ğŸ”„ é‡è¯•ç¬¬ {attempt} æ¬¡...")
                    time.sleep(2)
                else:
                    print("âš ï¸ è·³è¿‡å½“å‰å¸–å­ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª")
                    current_id -= 1
                    success = True  # è·³è¿‡è¿™ä¸ªID
        
        time.sleep(0.12)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
    
    return all_posts

def convert_relative_time(relative_time: str) -> str:
    """è½¬æ¢ç›¸å¯¹æ—¶é—´ä¸ºç»å¯¹æ—¶é—´"""
    if not relative_time:
        return ''
    
    # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºåŸºå‡†
    now = datetime.now()

    # æ­£åˆ™åŒ¹é…ä¸åŒæ—¶é—´å•ä½çš„å­—ç¬¦ä¸²
    patterns = {
        'minute': r'\s*(\d+)\s*åˆ†é’Ÿå‰',
        'hour': r'\s*(\d+)\s*å°æ—¶å‰',
        'day': r'\s*(\d+)\s*å¤©å‰',
        'week': r'\s*(\d+)\s*å‘¨å‰',
        'month': r'\s*(\d+)\s*æœˆå‰',
        'year': r'\s*(\d+)\s*å¹´å‰'
    }
    
    for unit, pattern in patterns.items():
        match = re.match(pattern, relative_time)
        if match:
            value = int(match.group(1))
            if unit == 'minute':
                delta = timedelta(minutes=value)
            elif unit == 'hour':
                delta = timedelta(hours=value)
            elif unit == 'day':
                delta = timedelta(days=value)
            elif unit == 'week':
                delta = timedelta(weeks=value)
            elif unit == 'month':
                delta = timedelta(days=value * 30)  # å‡è®¾ä¸€ä¸ªæœˆå¤§çº¦30å¤©
            elif unit == 'year':
                delta = timedelta(days=value * 365)  # å‡è®¾ä¸€å¹´æœ‰365å¤©
            
            # è®¡ç®—å¹¶è¿”å›ç›®æ ‡æ—¶é—´
            target_time = now - delta
            return target_time.strftime('%Y-%m-%d %H:%M:%S')

    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
    try:
        datetime.strptime(relative_time, '%Y-%m-%d %H:%M:%S')
        return relative_time
    except:
        return relative_time

def process_posts(posts):
    """å¤„ç†å¸–å­æ•°æ®ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
    processed = []
    
    for post in posts:
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            post_id = post.get('id', 0)
            content = post.get('content', '')
            post_code = post.get('post_code', '')
            class_code = post.get('class_code', '')
            class_name = post.get('class_name', '')
            time_str = post.get('time', '')
            good_count = post.get('good_count', 0)
            comment_count = post.get('comment_count', 0)
            root_code = post.get('root_code', None)
            
            # å¤„ç†æ—¶é—´æ ¼å¼
            formatted_time = convert_relative_time(time_str)
            
            processed.append({
                'id': post_id,
                'content': content,
                'post_code': post_code,
                'class_code': class_code,
                'class_name': class_name,
                'time': formatted_time,
                'good_count': good_count,
                'comment_count': comment_count,
                'Root_code': root_code
            })
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†å¸–å­å¤±è´¥: {e}")
            continue
    
    return processed

def save_to_csv(posts, append=True):
    """ä¿å­˜å¸–å­åˆ°CSVæ–‡ä»¶"""
    csv_path = './data/all.csv'
    
    if not posts:
        print("âš ï¸ æ²¡æœ‰æ–°å¸–å­éœ€è¦ä¿å­˜")
        return
    
    try:
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(posts)
        
        if append and os.path.exists(csv_path):
            # è¿½åŠ æ¨¡å¼ï¼šè¯»å–ç°æœ‰æ•°æ®
            existing_df = pd.read_csv(csv_path)
            
            # å»é‡ï¼šåŸºäºIDå»é‡
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['id'], keep='last')
            
            # æŒ‰IDæ’åº
            combined_df = combined_df.sort_values('id')
            
            print(f"ğŸ“Š åˆå¹¶æ•°æ®: åŸæœ‰ {len(existing_df)} æ¡ï¼Œæ–°å¢ {len(df)} æ¡ï¼Œå»é‡å {len(combined_df)} æ¡")
        else:
            # æ–°å»ºæ¨¡å¼
            combined_df = df.sort_values('id')
            print(f"ğŸ“Š æ–°å»ºæ•°æ®æ–‡ä»¶: {len(combined_df)} æ¡è®°å½•")
        
        # ä¿å­˜åˆ°CSV
        combined_df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {csv_path}")
        
        return len(combined_df)
        
    except Exception as e:
        print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        return 0

def main():
    """ä¸»å‡½æ•° - æ­£å¼çˆ¬å–æ¨¡å¼"""
    print("ğŸš€ å¼€å§‹æ­£å¼æ•°æ®çˆ¬å–ä»»åŠ¡...")
    
    # åŠ è½½é…ç½®
    config = load_crawl_config()
    last_crawled_id = config['last_crawl_id']
    
    # è·å–æœ€æ–°ID
    newest_id = get_newest_id()
    if not newest_id:
        print("âŒ æ— æ³•è·å–æœ€æ–°å¸–å­IDï¼Œçˆ¬å–ç»ˆæ­¢")
        return
    
    # ç¡®å®šçˆ¬å–èŒƒå›´
    if last_crawled_id == 0:
        # é¦–æ¬¡è¿è¡Œï¼šä»æœ€æ–°IDçš„å‰50æ¡å¼€å§‹çˆ¬å–
        start_id = max(1, newest_id - 50)
        end_id = newest_id
        print(f"ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œä»æœ€æ–°IDçš„å‰50æ¡å¼€å§‹çˆ¬å–")
        print(f"ğŸ“Š çˆ¬å–èŒƒå›´: {start_id} -> {end_id} (å…± {end_id - start_id} ä¸ªID)")
    else:
        # å¢é‡çˆ¬å–ï¼šä»ä¸Šæ¬¡çˆ¬å–çš„IDç»§ç»­å‘æœ€æ–°IDçˆ¬å–
        start_id = last_crawled_id
        end_id = newest_id
        print(f"ğŸ”„ å¢é‡çˆ¬å–ï¼Œä»ID {start_id} ç»§ç»­å‘ {end_id} çˆ¬å–")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
    if start_id >= end_id:
        print("âœ… æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€çˆ¬å–")
        return
    
    # çˆ¬å–å¸–å­ï¼šä»start_idå‘end_idçˆ¬å–ï¼ˆä»æ—§åˆ°æ–°ï¼‰
    posts = crawl_posts(start_id, end_id)
    
    if not posts:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°æ–°å¸–å­")
        return
    
    # å¤„ç†å¸–å­æ•°æ®
    processed_posts = process_posts(posts)
    
    # ä¿å­˜åˆ°CSV
    total_count = save_to_csv(processed_posts, append=True)
    
    # æ›´æ–°é…ç½®ï¼šè®°å½•æœ€åçˆ¬å–çš„ID
    config['last_crawl_id'] = end_id
    config['last_crawl_time'] = datetime.now().isoformat()
    config['total_crawled'] = config.get('total_crawled', 0) + len(processed_posts)
    
    save_crawl_config(config)
    
    print(f"ğŸ‰ æ­£å¼çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“ˆ æœ¬æ¬¡çˆ¬å–: {len(processed_posts)} æ¡æ–°å¸–å­")
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {total_count} æ¡")
    print(f"ğŸ†” ä¸‹æ¬¡ä»ID: {end_id} å¼€å§‹")
    print("ğŸ’¡ è¿™æ˜¯çœŸå®æ•°æ®ï¼Œæ¥è‡ªå°å–‡å­API")

if __name__ == '__main__':
    main()